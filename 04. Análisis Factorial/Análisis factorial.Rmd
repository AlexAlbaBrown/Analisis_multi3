---
title: "Análisis factorial"
author: "Daniel Czarnievicz"
output: pdf_document
header-includes:
   - \usepackage{mathrsfs}
   - \usepackage{fancyhdr}
   - \usepackage{multirow}
   - \usepackage{cancel}
   - \usepackage{float}
   - \pagestyle{fancy}
   - \everymath{\displaystyle}
   - \setlength{\parindent}{1em}
   - \setlength{\parskip}{1em}
   - \setlength{\headheight}{15pt}
   - \lhead{Análisis Factorial}
   - \rhead{Daniel Czarnievicz}
   - \DeclareMathOperator*{\argmin}{arg\,min}
   - \DeclareMathOperator*{\plim}{plim}
   - \DeclareMathOperator{\E}{\mathbf{E}}
   - \DeclareMathOperator{\V}{\mathbf{Var}}
   - \DeclareMathOperator{\Cov}{\mathbf{Cov}}
geometry: margin=1in
fontsize: 12pt
bibliography: References.bib
biblio-style: plain
nocite: |
   @RSLang, @tidyverse, @fnnpack, @wasserman2007all, @rencher1998multivariate, @james2013introduction
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
   echo = FALSE
   )
```

# Análisis Factorial

Se parte de una matriz de datos de la forma:
$$\mathbf{X}_{I \times J} = 
\begin{bmatrix}
x_{11} & x_{12} & \ldots & x_{1J} \\
x_{21} & x_{22} & \ldots & x_{2J} \\
\vdots & \vdots & \ddots & \vdots \\
x_{I1} & x_{I2} & \ldots & x_{IJ} \\
\end{bmatrix}$$

A partir de ella se definen dos espacios:

1. El espacio definido por la nube de las $N_I$ filas, el cual está incluido en $\mathbb{R}^{J}$ (dado que cada fila constituye un vector con $J$ componentes).

1. El espacio definido por la nube de las $N_J$ columnas, el cual está incluido en $\mathbb{R}^{I}$ (dado que cada columna constituye un vector con $I$ componentes).

El objetivo principal de un análisis factorial es eliminar la información redundante (reducción de dimensionalidad). Los resultados de un análisis factorial son: los ejes de incercia, y las coordenadas de los puntos sobre dichos ejes (llamados factores).

## Desarrollo por $N_I$

Trabajamos primero con la nube de puntos $N_I$, definida por las filas de la matriz $\mathbf{X}_{I \times J}$. Cada individuo está representado por un vector $\mathbf{x}_i = (x_{i1}, \, x_{i2}, \, \ldots, \, x_{iJ})' \in \mathbb{R}^J$. El objetivo es encontrar el conjunto de ejes ortonormados que maximicen la inercia proyectada sobre ellos. Al conjunto de dichas coordenadas sobre un eje de inercia se le llama *factor*.

Se definen las matrices diagonales $\mathbf{M}_{J \times J}$ y $\mathbf{D}_{I \times I}$ tales que:
$$\begin{array}{ccc}
\mathbf{M}_{J \times J} = 
\begin{bmatrix}
m_{1} & \ldots & 0 \\
\vdots & \ddots & \vdots \\
0 & \ldots & m_{J} \\
\end{bmatrix}
& &
\mathbf{D}_{I \times I} = 
\begin{bmatrix}
p_{1} & \ldots & 0 \\
\vdots & \ddots & \vdots \\
0 & \ldots & p_{I} \\
\end{bmatrix}
\end{array}$$

Estas matrices actuarán de pesos o métricas en cada análisis según la siguiente tabla:

|                  | Espacio        | Métrica      | Pesos        |
|------------------|:--------------:|:------------:|:------------:|
| Nube de filas    | $\mathbb{R}^J$ | $\mathbf{M}$ | $\mathbf{D}$ |
| Nube de columnas | $\mathbb{R}^I$ | $\mathbf{D}$ | $\mathbf{M}$ |

Dado que $\mathbf{M}$ es diagonal, la distancia entre dos puntos $i$ y $k$ de $N_I$ se calcula como:
$$d^2(i, \, k) = \sum\limits_{j = 1}^{J} (x_{ij} - x_{kj})^2 m_j$$

Sea $\mathbf{u}_s$ un vector director de un eje cualquiera de $\mathbb{R}^{J}$. Definimos el vetor de coordenadas proyectadas sobre $\mathbf{u}_s$, al que llamaremos $\mathbf{F}_s(i)$, como:
$$\mathbf{F}_s(i) = \mathbf{x}_i' \mathbf{M} \mathbf{u}_s$$
Nótese que $\mathbf{x}_i'$ es de forma $1 \times J$, $\mathbf{M}$ es una matriz de forma $J \times J$, $\mathbf{u}_s$ es de forma $J \times 1$. $\mathbf{F}_s(i)$ es un escalar. Este número representa la proyección del $i$-ésimo individuo sobre el eje de inercia $\mathbf{u}_s$. Visto en forma matricial:
$$\mathbf{F}_s = \mathbf{X} \mathbf{M} \mathbf{u}_s \,\,\, \text{con} s = 1, \, \ldots, \, J$$

Nótenes que omitimos la mención al $i$-ésimo individuo en $\mathbf{F}_s$. Esto se debe a que $\mathbf{X}$ es una matriz de tamaño $I \times J$, por lo que $\mathbf{F}_s$ es un vector de dimensión $I \times 1$, donde su $i$-ésima entrada es la proyección del $i$-ésimo individuo sobre el eje de inercia $\mathbf{u}_s$.

Podemos entonces definir la *inercia de la nube proyectada* como:
$$\text{inercia} = \mathbf{F}_s' \mathbf{D} \mathbf{F}_s$$
la cual podemos escribir, utilizando la definición de $\mathbf{F}_s$, como:
$$\text{inercia} = \mathbf{u}_s' \mathbf{M}' \mathbf{X}' \mathbf{D} \mathbf{X} \mathbf{M} \mathbf{u}_s$$

El objetivo, tal como fuera planteado, es hallar el eje $\mathbf{u} \in \mathbb{R}^J$ unitario en la métrica $\mathbf{M}$, que maximice la inercia. Es decir,
$$\max\limits_u \{ \text{inercia} \} = \max\limits_u \left\{ \mathbf{u}_s' \mathbf{M}' \mathbf{X}' \mathbf{D} \mathbf{X} \mathbf{M} \mathbf{u}_s \right\}$$

Si la métrica es euclidea, $\mathbf{M}$ es la matriz identidad, y el problema se reduce a hallar un vector $\mathbf{u}$ tal que $\mathbf{u}' \mathbf{u} = 1$ que maximice la inercia:
$$\max\limits_u \{ \text{inercia} \} = \max\limits_u \left\{ \mathbf{u}' \mathbf{X}' \mathbf{D} \mathbf{X} \mathbf{u} \right\}$$

Dado que $\mathbf{X}' \mathbf{D} \mathbf{X}$ es simétrica:

- $\mathbf{X}' \mathbf{D} \mathbf{X}$ es diagonalizable.

- $\exists \, \mathbf{U}$ matriz ortogonal (es decir, $\mathbf{U}' \mathbf{U} = \mathbf{U} \mathbf{U}' = \mathbf{I}$).

- $\mathbf{U} = ((u_j))$ son los vetores propios asociado al valor propio $\lambda_j$.

- se define $\mathbf{\Lambda}$ como la matriz diagonal de valores propios: $\Lambda = \text{diag}(\lambda_1, \, \ldots, \, \lambda_J)$ tales que 
$$\mathbf{U}' \mathbf{X}' \mathbf{D} \mathbf{X} \mathbf{U} = \mathbf{\Lambda} \Rightarrow \mathbf{X}' \mathbf{D} \mathbf{X} = \mathbf{U}' \mathbf{\Lambda} \mathbf{U}$$

- sus vectores propios forman una base ortonormal en $\mathbf{R}^J$.

## Desarrollo por $N_J$

## Interpretación de resultados

# Análisis de Componentes Principales

Partimos de una matriz de datos donde a $n$ individuos se le miden $p$ variables cuantiativas. Cada individuo puede entonces ser representado por un vector en $\mathbb{R}^p$, y cada variable puede ser representada por un vector en $\mathbb{R}^n$. El objetivo es reducir la cantidad de dimensiones necesarias para representar cada individuo, sin perder información (i.e. varianza) sobre ellos.

## Construcción de los ejes factoriales

**Maximizar la inercia capturada**

En esta visión del método, lo que se quiere es encontrar las combinaciones lineales de las variables originales que representan los datos de forma correcta. Se utilizan tantos ejes direccionales (nuevas variables) como sea necesario (máximo $p$). El primero de ellos se encuentra resolviendo el sistem de ecuaciones
$$\mathbf{z}_1 = \mathbf{X} \mathbf{\phi}_1$$
de forma de maximizar la varianza capturada por él. Es decir, de todas las posibles combinaciones lineales, buscamos aquella sobre la cual las proyecciones de los puntos originales estén más separadas. A los coeficientes $\mathbf{\phi}_1 = (\phi_{11}, \, \phi_{21}, \, \ldots, \, \phi_{p1})'$ les llamamos *loadings* de la primer componente principal, y les imponemos que $\sum\limits_{j = 1}^{p} \mathbf{\phi}_j^2 = 1$. Esto se debe a que, de no hacerlo, podríamos aumentarlos arbitrariamente, para aumentar así la varianza.

Para calcular los loadings, primero debemos estandarizar las variables. Luego debemos resolver el problema
$$\max\limits_{\phi_1} = \left\{ \frac{1}{n} \sum\limits_{i = 1}^{n} z_{i1}^2 \right\} = \max\limits_{\phi_1} = \left\{ \frac{1}{n} \sum\limits_{i = 1}^{n} \left( \sum\limits_{j = 1}^{p} \phi_{j1} x_{ij} \right)^2 \right\}$$
$$\text{sujeto a} \,\,\,\,\,\, \sum\limits_{j = 1}^{p} \mathbf{\phi}_j^2 = 1$$

Al vector $\mathbf{z}_1 = (z_{11}, \, z_{21}, \, \ldots, \, z_{n1})'$ le llamamos vector de *scores* de la primera componente principal. El score es el valor que la $i$-ésima observación toma en la proyección sobre el eje encontrado.

Para hallar la segunda componente principal, debemos hallar los scores que maximizan la varianza, siendo el nuevo vector ortogonal al vector $\mathbf{z}_1$.
$$\max\limits_{\phi_2} = \left\{ \frac{1}{n} \sum\limits_{i = 1}^{n} z_{i2}^2 \right\} = \max\limits_{\phi_2} = \left\{ \frac{1}{n} \sum\limits_{i = 1}^{n} \left( \sum\limits_{j = 1}^{p} \phi_{j2} x_{ij} \right)^2 \right\}$$
$$\text{sujeto a} \,\,\,\,\,\, \sum\limits_{j = 1}^{p} \mathbf{\phi}_j^2 = 1 \,\,\,\,\,\, \text{y} \,\,\,\,\,\, \langle \mathbf{z}_1, \, \mathbf{z}_2 \rangle= 0$$

Se procede de esta forma hasta hallar todos los ejes $\mathbf{z}_j$, hasta un máximo de $p$ ejes.

**Minimizar distancias al eje de proyección**

Otra forma de resolver el problema es minimizando la suma de las distancias al cuadrado entre los puntos originales y sus proyecciones sobre los ejes. Cuando hablamos de distancias, en este caso, nos referimos a las proyecciones (en sentido algebráico). Es decir, la distancia medida de forma tal que esta sea perpendicular al eje de inercia, no al eje $x_1$ o $x_2$. Si llamamos $r_i$ a la distancia del individuo $i$ al eje de proyección $a_1$, utilizamos el teorema de Pitágoras para ver que:
$$r_i^2 = ||\mathbf{x}_i||^2 - ||z_i \, \mathbf{a}_1||^2 = \mathbf{x}_i' \mathbf{x}_i - (z_i)^2 ||\mathbf{a}_i||^2 = \mathbf{x}_i' \mathbf{x}_i - z_i^2$$

Nuestro problema se puede plantear entonces como:
$$\min \left\{ \sum\limits_{i = 1}^{n} r_i^2 \right\} = \min \left\{ \sum\limits_{i = 1}^{n} \mathbf{x}_i'\mathbf{x}_i - \sum\limits_{i = 1}^{n}z_i^2 \right\} = \sum\limits_{i = 1}^{n} \mathbf{x}_i'\mathbf{x}_i - \max \left\{ \sum\limits_{i = 1}^{n}z_i^2 \right\}$$
donde utilizamos que $\sum\limits_{i = 1}^{n} \mathbf{x}_i'\mathbf{x}_i$ está fija.

Dado que la matriz de datos está centrada, $\sum\limits_{i = 1}^{n}z_i^2$ es la varianza muestral de los datos proyectados. Por lo tanto, queremos hallar los ejes incorrelados que resuelven:
$$\mathbf{z}_j = \mathbf{X} \mathbf{a}_j$$
Nuevamente, impondremos que $\mathbf{a}'_j\mathbf{a}_j = 1$.

Por último, notemos tambien que, dado que la matriz de datos está centrada,
$$\begin{array}{rcl}
\V(\mathbf{z}_1) & = & \V(\mathbf{X} \mathbf{a}_1) \\ \\
   & = & \frac{1}{n} \mathbf{z}_1' \mathbf{z}_1 \\ \\
   & = & \frac{1}{n} (\mathbf{X} \mathbf{a}_1)' (\mathbf{X} \mathbf{a}_1) \\ \\
   & = & \frac{1}{n} \mathbf{a}_1' \mathbf{X}' \mathbf{X} \mathbf{a}_1 \\ \\
   & = & \mathbf{a}_1' \left( \frac{1}{n} \mathbf{X}' \mathbf{X} \right) \mathbf{a}_1 \\ \\
   & = & \mathbf{a}_1' \mathbf{\Sigma}_{\mathbf{X}} \mathbf{a}_1
\end{array}$$

Por lo tanto, el problema de maximización lo podemos resolver con le siguiente Lagrangeano:
$$\mathscr{L}_1(\mathbf{a}_1) = \mathbf{a}_1' \mathbf{\Sigma}_{\mathbf{X}} \mathbf{a}_1 - \lambda_1 (\mathbf{a}_1'\mathbf{a}_1 - 1)$$

Cuya CPO está dada por:
$$\frac{\partial \mathscr{L}_2(\mathbf{a}_1)}{\partial \mathbf{a}_1'} = 2 \mathbf{\Sigma}_{\mathbf{X}} \mathbf{a}_1 - 2 \lambda_1 \mathbf{I}_p \mathbf{a}_1 = \mathbf{0}_p \Rightarrow \left( \mathbf{\Sigma}_{\mathbf{X}} - \lambda_1 \mathbf{I}_p \right) \mathbf{a}_1 = \mathbf{0}_p$$

Dado que lo que estamos buscando es un $\mathbf{a}_1 \neq \mathbf{0}_p$, resolver la CPO equivale a resolver
$$\det\left( \mathbf{\Sigma}_{\mathbf{X}} - \lambda_1 \mathbf{I}_p \right) = 0$$
por lo tanto, $\lambda$ es el valor propio de $\mathbf{\Sigma}_{\mathbf{X}}$ asociado al vector propio $\mathbf{a}_1$. Volviendo al análisis de la varianza, tenemos que:
$$\V(\mathbf{z}_1) = \mathbf{a}_1' \mathbf{\Sigma}_{\mathbf{X}} \mathbf{a}_1 = \mathbf{a}_1' \lambda_1 \mathbf{a}_1 = \lambda_1 \mathbf{a}_1' \mathbf{a}_1 = \lambda_1$$

Concluimos entonces que, para maximizar la varianza (o minimizar las distancias $r_i$), debemos tomar el mayor valor propio asociado a $\mathbf{\Sigma}_{\mathbf{X}}$, $\lambda_1$, y su correspondiente vector propio asociado, $\mathbf{a}_1$.

Luego, para hallar $\mathbf{z}_2$, procedemos de forma análoga, pero agregando la restricción adicional $\Cov(\mathbf{a}_1, \, \mathbf{a}_2) = 0$. Por lo tanto, el Lagrangeano asociado al problema será:
$$\mathscr{L}_2(\mathbf{a}_2) = \mathbf{a}_2' \mathbf{\Sigma}_{\mathbf{X}} \mathbf{a}_2 - \lambda_2 (\mathbf{a}_2' \mathbf{a}_2 - 1) - \delta (\mathbf{a}_2' \mathbf{a}_1 - 0 )$$

Su CPO será:
$$\begin{array}{rcl}
\frac{\partial \mathscr{L}_2(\mathbf{a}_2)}{\partial \mathbf{a}_2'} & = & 2 \mathbf{\Sigma}_{\mathbf{X}} \mathbf{a}_2 - 2 \lambda_2 \mathbf{a}_2 - \delta \mathbf{a}_1 \\ \\
   & = & 2 \mathbf{a}_1' \mathbf{\Sigma}_{\mathbf{X}} \mathbf{a}_2 - 2 \lambda_2 \mathbf{a}_1' \mathbf{a}_2 - \delta \mathbf{a}_1' \mathbf{a}_1 \\ \\
   & = & 2 \mathbf{a}_1' \mathbf{\Sigma}_{\mathbf{X}} \mathbf{a}_2 - 2 \lambda_2 \underbrace{ \mathbf{a}_1' \mathbf{a}_2 }_{ = 0} - \delta \underbrace{ \mathbf{a}_1' \mathbf{a}_1 }_{ = 1} \\ \\
   & = & 2 \mathbf{a}_1' \mathbf{\Sigma}_{\mathbf{X}} \mathbf{a}_2 - \delta = \mathbf{0}_p
\end{array}$$
Tenemos entonces que el máximo se halla cuando 
$$\delta = 2 \mathbf{a}_1' \mathbf{\Sigma}_{\mathbf{X}} \mathbf{a}_2 = 2 \mathbf{a}_2' \mathbf{\Sigma}_{\mathbf{X}} \mathbf{a}_1$$
por lo tanto,
$$\frac{\partial \mathscr{L}_2(\mathbf{a}_2)}{\partial \mathbf{a}_2'} = 2 \mathbf{\Sigma}_{\mathbf{X}} \mathbf{a}_2 - 2 \lambda_2 \mathbf{I}_p \mathbf{a}_2 = \mathbf{0}_p \Rightarrow \left( \mathbf{\Sigma}_{\mathbf{X}} - \lambda_2 \mathbf{I}_p \right) \mathbf{a}_2 = \mathbf{0}_p$$
lo cual implica elegir el segundo mayor valor propio de $\mathbf{\Sigma}_{\mathbf{X}}$, y $\mathbf{a}_2$ será el vector propio asociado a dicho valor propio.

Este procedimiento se repite $p$ veces, restringiendo a que cada nuevo vector $\mathbf{z}_j$ sea ortogonal al anterior. Se obtiene así la matriz ortogonal $\mathbf{A}_{p \times p} = (\mathbf{a}_1, \, \mathbf{a}_2, \, \ldots, \, \mathbf{a}_p)$.

## Relaciones entre ejes

## Elección de la cantidad de ejes

# Análisis de Correspondencias Simples



# Análisis de Correspondencias Múltiple


# Referencias
