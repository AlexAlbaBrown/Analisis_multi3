---
title: "Análisis discriminante"
author: "Daniel Czarnievicz"
output: pdf_document
header-includes:
   - \usepackage{mathrsfs}
   - \usepackage{fancyhdr}
   - \usepackage{multirow}
   - \usepackage{cancel}
   - \usepackage{float}
   - \pagestyle{fancy}
   - \everymath{\displaystyle}
   - \setlength{\parindent}{1em}
   - \setlength{\parskip}{1em}
   - \setlength{\headheight}{15pt}
   - \lhead{Análisis Discriminante}
   - \rhead{Daniel Czarnievicz}
   - \DeclareMathOperator*{\argmin}{arg\,min}
geometry: margin=1in
fontsize: 12pt
bibliography: References.bib
biblio-style: plain
nocite: |
   @RSLang, @tidyverse, @fnnpack, @wasserman2007all, @rencher1998multivariate, @james2013introduction
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
   echo = FALSE
   )
```

# Descripción general

El análisis discriminante es una técnica supervisada con finalidades de descripción (analizar la existencia de diferencias entre grupos), predicción (clasificar nuevas observaciones) y re-clasificación. El problema consiste en construir un modelo que permita discriminar las observaciones según el grupo poblacional al que pertenecen. A la $i$-ésima observación se le miden $p$ características, las cuales componen el vector $\mathbf{x}_i' = (x_{i1}, \, x_{i2}, \, \ldots, \, x_{ip})$. Se asume que existen $k$ grupos en la población.

# Reglas de decisión

Existen distintas reglas de decisión para la asignación de observaciones a grupos.

## Minimizar la probabilidad de error

La regla de decisión será aquella que minimize la probabilidad total de error. Supongamos que una población $P$ está sub-dividida en $k$ grupos excluyentes. Llamaremos $f_k(x)$ a la densidad de $x$, si $x$ pertenece al $k$-ésimo grupo. El objetivo es encontrar una partición del espacio muestral $R$, tal que asigne $x$ al grupo $k \Leftrightarrow x \in r_x$.

Llamaremos $\Pr(g'|g)$ al error de clasificar en el grupo $g'$ una observación perteneciente al grupo $g$. Entonces:
$$\Pr(g'|g) = \int\limits_{R_{g'}} f_g(x) dx$$

Por lo tanto, la probabilidad de clasificar erróneamente a todas las observaciones provenientes del grupo $g$ está dada por:
$$\Pr(g) = \sum\limits_{\substack{ g' = 1 \\ g' \neq g }}^{k} \Pr(g'|g) = 1 - \Pr(g|g)$$

De esta forma entonces, la probabilidad total de clasificación errónea está dada por:
$$\Pr(R, \, f) = \sum\limits_{g = 1}^{k} \pi_g \Pr(g)$$
donde $\pi_g$ es la probabilidad a priori de que $i$ pertenzca a al grupo $g$.

## Principio de máxima verosimilitud

El pricipio de clasificación por máxima verosimilitud consiste en asignar la observación $i$ a la población donde el vector observado $\mathbf{x}_i'$ tenga mayor verosimilitud de ocurrir. Es decir, se asigna $i$ al grupo $g$, sí y solo si:
$$f(\mathbf{x}_i|g) > f(\mathbf{x}_i|g') \,\,\, \forall g' \neq g \Leftrightarrow \Pr(\mathbf{x}_i|g) > \Pr(\mathbf{x}_i|g') \,\,\, \forall g' \neq g \Leftrightarrow \frac{ f(\mathbf{x}_i|g) }{ f(\mathbf{x}_i|g') } > 1$$

## Principio de probabilidad a posteriori

La regla consiste en asignar la observación $i$ a la población con mayor probabilidad a posteriori (la probailidad de que $i$ pertenzca a $g$, dado $\mathbf{x}_i$). Utilizando el Teorema de Bayes, tenemos que la probabilidad a posteriori está dada por:
$$\Pr(i \in g | \mathbf{x} = \mathbf{x}_i) = \frac{ \pi_g \Pr(\mathbf{x}_i | g) }{ \Pr(\mathbf{x}_i) } = \frac{ \pi_g \Pr(\mathbf{x}_i | g) }{ \sum\limits_{g' = 1}^{k} \pi_g' \,\, \Pr(\mathbf{x}_i | g') } = \frac{ \pi_g f(\mathbf{x}_i | g) }{ \sum\limits_{g' = 1}^{k} \pi_{g'} \,\, f(\mathbf{x}_i | g') }$$
donde $\pi_g$ es la probailidad previa de que $i \in g$. Salvo que información adicional sugiera lo contrario, $\pi_g$ se estimad como la proporción de observaciones en $\mathbf{X}$ que pertenecen a la clase $g$. Esto es, $\hat{\pi}_g = n_g / n$, siendo $n$ la cantidad total de observaciones, y $n_g$ la cantidad de observaciones con variable de respuesta igual a la etiqueta de la clase $g$.

De esta forma, la observación $i$ se asignará al grupo $g$, sí y solo sí:
$$\Pr(i \in g | \mathbf{x} = \mathbf{x}_i) > \Pr(i \in g' | \mathbf{x} = \mathbf{x}_i) \,\,\, \forall g' \neq g$$

**Normalidad**

Si $\mathbf{x}_i \sim \text{N}_p(\mathbf{\mu}, \, \mathbf{\Sigma})$ su función de densidad viene dada por:
$$f(\mathbf{x}_i | g) = \frac{ 1 }{ (2 \pi)^{^p/_2} | \mathbf{\Sigma}_g |^{^1/_2} } \, \exp \left\{ - \frac{1}{2} (\mathbf{x}_i - \mathbf{\mu}_g)' \mathbf{\Sigma}_g^{-1} (\mathbf{x}_i - \mathbf{\mu}_g) \right\}$$

La densidad puede estimarse utilizando los estimadores MV de $\mathbf{\mu}_g$ y $\mathbf{\Sigma}_g$, $\mathbf{\bar{x}}_g$ y $\mathbf{S}_g$ respectivamente, para obtener:
$$\hat{f}(\mathbf{x}_i | g) = \frac{ 1 }{ (2 \pi)^{^p/_2} | \mathbf{S}_g |^{^1/_2} } \, \exp \left\{ - \frac{1}{2} (\mathbf{x}_i - \mathbf{\bar{x}}_g)' \mathbf{S}_g^{-1} (\mathbf{x}_i - \mathbf{\bar{x}}_g) \right\}$$

Si aplicamos el supuesto de normalidad a la probabilidad posteriori, obtenemos que:
$$\Pr(i \in g | \mathbf{x} = \mathbf{x}_i) = \frac{ \pi_g \,\, | \mathbf{\Sigma}_g |^{-1/2} \exp\left\{ (-1/2) \, D^2_{ig} \right\} }{ \sum\limits_{g' = 1}^{k} \pi_{g'} \,\, | \mathbf{\Sigma}_{g'} |^{-1/2} \exp\left\{ (-1/2) \, D^2_{ig'} \right\} }$$
donde $D^2_{ig}$ y $D^2_{ig'}$ son la distancia de Mahalanobis entre la observación $i$ y los grupos $g$ y $g'$ respectivamente. Utilizando los estimadores de $\mathbf{\mu}_g$ y $\mathbf{\Sigma}_g$ mencionadas anteriormente, obtenemos que:
$$\hat{\Pr}(i \in g | \mathbf{x} = \mathbf{x}_i) = \frac{ \hat{\pi}_g \,\, | \mathbf{S}_g |^{-1/2} \exp\left\{ (-1/2) \, \hat{D}^2_{ig} \right\} }{ \sum\limits_{g' = 1}^{k} \hat{\pi}_{g'} \,\, | \mathbf{S}_{g'} |^{-1/2} \exp\left\{ (-1/2) \, \hat{D}^2_{ig'} \right\} }$$
y la observación $i$ se asignará al grupo $g$, sí, y solo si se cumple que:
$$\hat{\pi}_g \,\, | \mathbf{S}_g |^{-1/2} \exp\left\{ (-1/2) \, \hat{D}^2_{ig} \right\} > \hat{\pi}_{g'} \,\, | \mathbf{S}_{g'} |^{-1/2} \exp\left\{ (-1/2) \, \hat{D}^2_{ig'} \right\} \,\,\, \forall g' \neq g$$

# Costos

Existen situaciones en las que el error de clasificación es más costo para algunos grupos que para otros. La regla de decisión puede modificarse de forma tal de contemplar estas situaciones de la siguiente forma. Se define un costo para cada error de clasificación, $c(g|g')$. Luego, se asigna $i$ al grupo $g$ sí, y solo si, se cumple que:
$$\frac{ f(\mathbf{x}_i | g) }{ f(\mathbf{x}_i | g') } > \frac{ \pi_{g'} \, c(g | g') }{ \pi_{g} \, c(g' | g) } \,\,\, \forall g' \neq g$$

# Errores de clasificación

## Tasa de error aparente

Luego de elegida una regla de clasificación, se utilizan los $n$ datos para construir la función discriminante y clasificar las observaciones. Una vez clasificadas, se calcula la *tasa de error aparente*,
$$e_{i, \, app} = \frac{m_i}{n_i}$$
donde $m_i$ es la cantidad de observaciones clasificadas errónemente, de las $n_i$ observaciones asignadas al grupo $g_i$.

## LOOCV

*Leave-one-out cross-validation* consiste en:

- apartar una observación de la muestra.  
- construir la función discriminante con las $n - 1$ observaciones restantes.  
- clasificar laa observación apartada y registrar si dicha observación fue correcta o incorrectamente clasificada.  
- repetir para cada una de las $n$ observaciones  

La proporción de observaciones mal clasificadas dentro de cada grupo se define como:
$$e_{i, \, c} = \frac{a_i}{n_i}$$

# Funciones discriminantes

Existen distintas formas de construir una función discriminante. El AD busca:

- examinar la separación entre grupos.  
- encontrar el subconjunto de las variables originales que separa los grupos tan bien como el conjunto original.  
- determinar cuál variable es la que tiene mayor constribución a la discriminación.  
- interpretar las nuevas dimensiones rrepresentadas por las funciones discriminantes.  
- re-clasificar individuos.  
- predecir (asignar nuevos individuos a un grupo).  

## AD factorial

El AD factorial consiste en encontrar las combinaciones lineales de los datos, $\mathbf{Z} = \mathbf{X} \mathbf{u}$, que tengan mayor poder discriminante para clasificar las observaciones en $k$ grupos. Las nuevas variables, susceptibles de separar lo máximo posible los $k$ grupos, representan un compromiso entre mínima inercia intra-clase (grupos homogéneos), y máxima incercia inter-clase (grupos separados). Por lo tanto, el objetivo es contrar las variables que maximizen el cociente entre ambas inercias:
$$\frac{ \mathbf{u}' \mathbf{B} \mathbf{u} }{ \mathbf{u}' \mathbf{W} \mathbf{u} }$$
donde $\mathbf{u}$ son los ejes de inercia que maximizan dicho conciente, y $\mathbf{Z}$ son las coordenadas de los individuos en las nuevas variables (es decir, la proyección de $\mathbf{X}$ en los ejes de inercia). Por lo tanto, el problema a resolver es:
$$\max\left\{ \frac{ \mathbf{u}' \mathbf{B} \mathbf{u} }{ \mathbf{u}' \mathbf{W} \mathbf{u} } \right\}$$

Condiciones de primer orden (CPO):
$$\frac{ \partial (\bullet) }{ \partial \mathbf{u}' } = \mathbf{0} \Rightarrow \frac{ 2 \mathbf{B} \mathbf{u} (\mathbf{u}'\mathbf{W} \mathbf{u}) - 2 (\mathbf{u}' \mathbf{B} \mathbf{u}) \mathbf{W} \mathbf{u} }{ (\mathbf{u}'\mathbf{W}\mathbf{u})^2 } = \mathbf{0}$$

Dado que $\mathbf{X}$ es una matriz de $n \times p$, $\mathbf{W}$ y $\mathbf{B}$ son matrices de $n \times n$, mientras que $\mathbf{u}$ es un vector de forma $n \times 1$, por lo que $\mathbf{u}'$ tiene forma $1 \times n$. Tenemos entonces que $\mathbf{u}' \mathbf{B} \mathbf{u}$ y $\mathbf{u}' \mathbf{W} \mathbf{u}$ son escalares. Por lo tanto, las siguientes manipulaciones son válidas:
$$\frac{ 2 \mathbf{B} \mathbf{u} (\mathbf{u}'\mathbf{W} \mathbf{u}) - 2 (\mathbf{u}' \mathbf{B} \mathbf{u}) \mathbf{W} \mathbf{u} }{ (\mathbf{u}'\mathbf{W}\mathbf{u})^2 } = \mathbf{0} \Rightarrow \frac{ \mathbf{B} \mathbf{u} (\mathbf{u}'\mathbf{W} \mathbf{u}) }{ (\mathbf{u}'\mathbf{W}\mathbf{u})^2 } = \frac{ (\mathbf{u}' \mathbf{B} \mathbf{u}) \mathbf{W} \mathbf{u} }{ (\mathbf{u}'\mathbf{W}\mathbf{u})^2 } \Rightarrow$$
$$\Rightarrow \mathbf{B} \mathbf{u} = \mathbf{W} \mathbf{u} \, \frac{ \mathbf{u}' \mathbf{B} \mathbf{u} }{ \mathbf{u}' \mathbf{W} \mathbf{u} } \Rightarrow \mathbf{W}^{-1} \mathbf{B} \mathbf{u} = \mathbf{u} \, \frac{ \mathbf{u}' \mathbf{B} \mathbf{u} }{ \mathbf{u}' \mathbf{W} \mathbf{u} }$$
dond el último paso lo podemos hacer dado que sabemos que $\mathbf{W}$ es invertible. Luego, si definimos $\lambda = \frac{ \mathbf{u}' \mathbf{B} \mathbf{u} }{ \mathbf{u}' \mathbf{W} \mathbf{u} }$ obtenemos que:
$$\mathbf{W}^{-1} \mathbf{B} \mathbf{u} = \lambda \, \mathbf{u}$$

Hallamos entonces que $\mathbf{u}$ es entonces el vector propio asociado al máximo valor propio de la matriz $\mathbf{W}^{-1} \mathbf{B}$, mientras que el valor propio $\lambda$ representa la máxima varianza inter-clases $\mathbf{u}' \mathbf{B} \mathbf{u}$ de la nueva variable $Z$.

En total, pueden hallarse $r = \min(k - 1, \, p)$ valores y vectores propios no nulos. Llamamos $\lambda_1, \, \lambda_2, \, \ldots, \, \lambda_r$ a los valores propios, y $\mathbf{u}_1, \, \mathbf{u}_2, \ldots, \, \mathbf{u}_r$ a los vectores propios asociados, tales que $\lambda_1 > \lambda_2 > \ldots > \lambda_r$. Las variables $\mathbf{Z}_j = \mathbf{X} \mathbf{u}_j$ proporcionan la máxima separación para discriminar entre los $k$ grupos. Esta variables son incorreladas, dado que se construyen de forma secuencial y de manera ortogonal. Es decir, $\mathbf{u}_1$ es tal que la proyección de los grupos sobre si misma tiene máxima separación relativa. La segunda dirección, $\mathbf{u}_2$, se construye de forma tal de que la separación entre grupos sea máxima, y sea ortogonal a la dirección determinada por $\mathbf{u}_1$ (esto es, $\langle \mathbf{u}_1, \, \mathbf{u}_2 \rangle = 0$).

Para determinar con cuántas funciones discriminantes trabajar se calcula la variación explicada por ellas, siendo $\frac{ \lambda_1 }{ \sum\limits_{j = 1}^{r} \lambda_j }$ la de la primera, $\frac{ \lambda_1 + \lambda_2 }{ \sum\limits_{j = 1}^{r} \lambda_j }$ la primera y la segunda juntas, y así sucesivamente. La correlación entre las variables orginales y las combinaciones lineales establece la importancia de cada una de las variables originales para discriminar.

## AD probabilístico (distribución normal)

Se asume que cada grupo tiene una distribución normal $p$-variada.
$$\begin{array}{c}
\mathbf{x}_1 \sim \text{N}_p(\mathbf{\mu}_1, \, \mathbf{\Sigma}_1) \\
\mathbf{x}_2 \sim \text{N}_p(\mathbf{\mu}_2, \, \mathbf{\Sigma}_2) \\
\vdots \\
\mathbf{x}_k \sim \text{N}_p(\mathbf{\mu}_k, \, \mathbf{\Sigma}_k) \\
\end{array}$$

### AD lineal

Adicionalmente, se asume que las matrices de covarianzas son iguales en todos los grupos: 
$$\mathbf{\Sigma}_1 = \mathbf{\Sigma}_2 = \ldots = \mathbf{\Sigma}_k$$

Sabemos que para aplicar la regla de probabilidad a posteriori, se debe maximizar la probabilidad condicional $\Pr(i \in g | \mathbf{x} = \mathbf{x}_i)$. Sabemos tambien que maximizar dicha probabilidad condicional es equivalente a maximizar $\pi_g \, \exp\left\{ - \frac{1}{2} D_{ig}^2 \right\}$. Definimos la función $L_{ig}$ como el logaritmo de nuestra función objetivo, esto es:
$$\begin{array}{rcl}
L_{ig} & = & \log \pi_g - \frac{1}{2} D_{ig}^2 \\ \\
   & = & \log \pi_g - \frac{1}{2} \left[ (\mathbf{x}_i - \mathbf{\mu}_g)' \mathbf{\Sigma}^{-1} ((\mathbf{x}_i - \mathbf{\mu}_g)) \right] \\ \\
   & = & \log \pi_g - \frac{1}{2} \mathbf{x}'_i \, \mathbf{\Sigma}^{-1} \, \mathbf{x}_i + \frac{1}{2} \mathbf{\mu}'_g \, \mathbf{\Sigma}^{-1} \, \mathbf{x}_i + \frac{1}{2} \mathbf{x}'_i \, \mathbf{\Sigma}^{-1} \, \mathbf{\mu}_g - \frac{1}{2} \mathbf{\mu}'_g \, \mathbf{\Sigma}^{-1} \, \mathbf{\mu}_g
\end{array}$$
dado que $\mathbf{\mu}_g$ y $\mathbf{\Sigma}$ son desconocidos, utilizamos sus estimadores MV. Adicionalmente, dado que $\mathbf{\Sigma}$ es una matrix simétrica, tenemos que $\frac{1}{2} \mathbf{\mu}'_g \, \mathbf{\Sigma}^{-1} \, \mathbf{x}_i + \frac{1}{2} \mathbf{x}'_i \, \mathbf{\Sigma}^{-1} \, \mathbf{\mu}_g = \mathbf{\mu}'_g \, \mathbf{\Sigma}^{-1} \, \mathbf{x}_i$. Por útimo, dado que el vector $\mathbf{x}_i$ es el mismo para todos los grupo, el término $\mathbf{x}'_i \, \mathbf{S}^{-1} \, \mathbf{x}_i$ puede descartarse ya que es constante. De esta forma, obtenemos que la función $L_{ig}$ está dada por
$$L_{ig} = \log \hat{\pi}_g + \mathbf{\bar{x}}'_g \, \mathbf{S}^{-1} \, \mathbf{x}_i - \frac{1}{2} \mathbf{\bar{x}}'_g \, \mathbf{S}^{-1} \, \mathbf{\bar{x}}_g$$

La función $L_{ig}$ es la conocida como **función discriminante lineal**. El valor que $L_{ig}$ toma es conocido como el **score** de la observación $i$ en el grupo $g$. La misma es lineal en las $x$.

La función que permite determinar en qué grupo clasificar una observación es llamada **función de la clasificación**. Para hallar, partimos de la desigualdad derivada en la sección de reglas de decisión: la observación $i$ se asigna al grupo $g$, sí, y solo si,
$$\Pr(i \in g | \mathbf{x} = \mathbf{x}_i) > \Pr(i \in g' | \mathbf{x} = \mathbf{x}_i) \,\,\, \forall g \neq g'$$

Para el caso de la normalidad con igualdad de matrices de covarianzas, sabemos que esto es equivalente a asignar la observación $i$ al grupo $g$ si se cumple que:
$$\hat{\pi}_g \, \hat{D}^2_{ig} > \hat{\pi}_{g'} \, \hat{D}^2_{ig'} \,\,\, \forall g \neq g'$$

Por lo tanto, para hallar la función de clasificación, debemos comparar las funciones discriminantes para los grupo $g$ y $g'$. Esto es, $i$ se asignará al grupo $g$ si se cumple:
$$\begin{array}{rcl}
L_{ig} & > & L_{ig'} \\ \\
\log \hat{\pi}_g + \mathbf{\bar{x}}'_g \, \mathbf{S}^{-1} \, \mathbf{x}_i - \frac{1}{2} \mathbf{\bar{x}}'_g \, \mathbf{S}^{-1} \, \mathbf{\bar{x}}_g & > & \log \hat{\pi}_{g'} + \mathbf{\bar{x}}'_{g'} \, \mathbf{S}^{-1} \, \mathbf{x}_i - \frac{1}{2} \mathbf{\bar{x}}'_{g'} \, \mathbf{S}^{-1} \, \mathbf{\bar{x}}_{g'} \\ \\
\mathbf{\bar{x}}'_g \, \mathbf{S}^{-1} \, \mathbf{x}_i - \mathbf{\bar{x}}'_{g'} \, \mathbf{S}^{-1} \, \mathbf{x}_i - \frac{1}{2} \mathbf{\bar{x}}'_g \, \mathbf{S}^{-1} \, \mathbf{\bar{x}}_g + \frac{1}{2} \mathbf{\bar{x}}'_{g'} \, \mathbf{S}^{-1} \, \mathbf{\bar{x}}_{g'} & > & \log \hat{\pi}_{g'} - \log \hat{\pi}_g \\ \\
\left( \mathbf{\bar{x}}'_g - \mathbf{\bar{x}}'_{g'} \right) \mathbf{S}^{-1} \, \mathbf{x}_i - \frac{1}{2} \mathbf{\bar{x}}'_g \, \mathbf{S}^{-1} \, \mathbf{\bar{x}}_g + \frac{1}{2} \mathbf{\bar{x}}'_{g'} \, \mathbf{S}^{-1} \, \mathbf{\bar{x}}_{g'} & > & \log \left( \frac{ \hat{\pi}_{g'} }{ \hat{\pi}_{g} } \right) \\ \\
\left( \mathbf{\bar{x}}'_g - \mathbf{\bar{x}}'_{g'} \right) \mathbf{S}^{-1} \, \mathbf{x}_i - \frac{1}{2} \left[ \mathbf{\bar{x}}'_g \, \mathbf{S}^{-1} \, \mathbf{\bar{x}}_g - \mathbf{\bar{x}}'_{g'} \, \mathbf{S}^{-1} \, \mathbf{\bar{x}}_{g'} \right] & > & \log \left( \frac{ \hat{\pi}_{g'} }{ \hat{\pi}_{g} } \right) \\ \\
\left( \mathbf{\bar{x}}'_g - \mathbf{\bar{x}}'_{g'} \right) \mathbf{S}^{-1} \, \mathbf{x}_i - \frac{1}{2} \left[ \left( \mathbf{\bar{x}}'_g - \mathbf{\bar{x}}'_{g'} \right) \mathbf{S}^{-1} \left( \mathbf{\bar{x}}_g - \mathbf{\bar{x}}_{g'} \right) \right] & > & \log \left( \frac{ \hat{\pi}_{g'} }{ \hat{\pi}_{g} } \right) \\ \\
\left( \mathbf{\bar{x}}'_g - \mathbf{\bar{x}}'_{g'} \right) \mathbf{S}^{-1} \left[ \mathbf{x}_i - \frac{1}{2} \left( \mathbf{\bar{x}}_g - \mathbf{\bar{x}}_{g'} \right) \right] & > & \log \left( \frac{ \hat{\pi}_{g'} }{ \hat{\pi}_{g} } \right) \\ \\
\underbrace { \left( \mathbf{\bar{x}}_g - \mathbf{\bar{x}}_{g'} \right)' \mathbf{S}^{-1} \left[ \mathbf{x}_i - \frac{1}{2} \left( \mathbf{\bar{x}}_g - \mathbf{\bar{x}}_{g'} \right) \right] }_{ L_{igg'} } & > & \log \left( \frac{ \hat{\pi}_{g'} }{ \hat{\pi}_{g} } \right)
\end{array}$$

Por lo tanto, la región de clasificación queda determinada por los $k - 1$ hiperplanos definidos por las inequaciones de la forma
$$R_{gg'}: \,\, L_{igg'} > \log \left( \frac{ \hat{\pi}_{g'} }{ \hat{\pi}_{g} } \right)$$
Los bordes de estas regiones están determinadas por las ecuaciones $L_{igg'} = \log \left( \frac{ \hat{\pi}_{g'} }{ \hat{\pi}_{g} } \right)$, y en ellos, no se cuenta con regla de decisión.

## AD probabilístico (distribución desconocida)


# Referencias
