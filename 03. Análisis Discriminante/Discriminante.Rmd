---
title: "Análisis discriminante"
author: "Daniel Czarnievicz"
output: pdf_document
header-includes:
   - \usepackage{mathrsfs}
   - \everymath{\displaystyle}
   - \setlength{\parindent}{1em}
   - \setlength{\parskip}{1em}
   - \usepackage{fancyhdr}
   - \pagestyle{fancy}
   - \lhead{Cluster Analysis}
   - \rhead{Daniel Czarnievicz}
   - \usepackage{multirow}
   - \usepackage{cancel}
   - \usepackage{float}
   - \DeclareMathOperator*{\argmin}{arg\,min}
geometry: margin=1in
fontsize: 12pt
bibliography: References.bib
biblio-style: plain
nocite: |
   @RSLang, @tidyverse, @fnnpack, @wasserman2007all, @rencher1998multivariate, @james2013introduction
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
   echo = FALSE
   )
```

# Descripción general

El análisis discriminante es una técnica con finalidades de descripción (analizar la existencia de diferencias entre grupos), predicción (clasificar nuevas observaciones) y re-clasificación. El problema consiste en construir un modelo que permita discriminar las observaciones según el grupo poblacional al que pertenecen. A la $i$-ésima observación se le miden $p$ características, las cuales componen el vector $\mathbf{x}_i' = (x_{i1}, \, x_{i2}, \, \ldots, \, x_{ip})$. Se asume que existen $k$ grupos en la población.

## Reglas de decisión

Existen distintas reglas de decisión para la asignación de observaciones a grupos.

### Minimizar la probabilidad de error

La regla de decisión será aquella que minimize la probabilidad total de error. Supongamos que una población $P$ está sub-dividida en $k$ grupos excluyentes. Llamaremos $f_k(x)$ a la densidad de $x$, si $x$ pertenece al $k$-ésimo grupo. El objetivo es encontrar una partición del espacio muestral $R$, tal que asigne $x$ al grupo $k \Leftrightarrow x \in r_x$.

Llamaremos $\Pr(g'|g)$ al error de clasificar en el grupo $g'$ una observación perteneciente al grupo $g$. Entonces:
$$\Pr(g'|g) = \int\limits_{R_{g'}} f_g(x) dx$$

Por lo tanto, la probabilidad de clasificar erróneamente a todas las observaciones provenientes del grupo $g$ está dada por:
$$\Pr(g) = \sum\limits_{\substack{ g' = 1 \\ g' \neq g }}^{k} \Pr(g'|g) = 1 - \Pr(g|g)$$

De esta forma entonces, la probabilidad total de clasificación errónea está dada por:
$$\Pr(R, \, f) = \sum\limits_{g = 1}^{k} \pi_g \Pr(g)$$
donde $\pi_g$ es la probabilidad a priori de que $i$ pertenzca a al grupo $g$.

### Principio de máxima verosimilitud

El pricipio de clasificación por máxima verosimilitud consiste en asignar la observación $i$ a la población donde el vector observado $\mathbf{x}_i'$ tenga mayor verosimilitud de ocurrir. Es decir, se asigna $i$ al grupo $g$, sí y solo si:
$$f(\mathbf{x}_i|g) > f(\mathbf{x}_i|g') \,\,\, \forall g' \neq g \Leftrightarrow \Pr(\mathbf{x}_i|g) > \Pr(\mathbf{x}_i|g') \,\,\, \forall g' \neq g \Leftrightarrow \frac{ f(\mathbf{x}_i|g) }{ f(\mathbf{x}_i|g') } > 1$$

### Principio de probabilidad a posteriori

La regla consiste en asignar la observación $i$ a la población con mayor probabilidad a posteriori (la probailidad de que $i$ pertenzca a $g$, dado $\mathbf{x}_i$). Utilizando el Teorema de Bayes, tenemos que la probabilidad a posteriori está dada por:
$$\Pr(i \in g | \mathbf{x} = \mathbf{x}_i) = \frac{ \pi_g \Pr(\mathbf{x}_i | g) }{ \Pr(\mathbf{x}_i) } = \frac{ \pi_g \Pr(\mathbf{x}_i | g) }{ \sum\limits_{g'1 = 1}^{k} \pi_g' \Pr(\mathbf{x}_i | g') } = \frac{ \pi_g f(\mathbf{x}_i | g) }{ \sum\limits_{g'1 = 1}^{k} \pi_g' f(\mathbf{x}_i | g') }$$

De esta forma, la observación $i$ se asignará al grupo $g$, sí y solo sí:
$$\Pr(i \in g | \mathbf{x} = \mathbf{x}_i) > \Pr(i \in g' | \mathbf{x} = \mathbf{x}_i) \,\,\, \forall g' \neq g$$

# Referencias
